{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be following the presentation here: https://tminka.github.io/papers/logreg/minka-logreg-old.pdf.\n",
    "\n",
    "The logistic loss is convex so we'll use a convex optimization process called gradient descent. In particular, we will use a variant called Newton's Method that uses the 2nd derivative of the loss function to set the learning rate. To start, here are some primitives:\n",
    "\n",
    "<b>$L = \\sum y * ln(p) + (1-y)*ln(1-p)$</b>\n",
    "\n",
    "\n",
    "<b> $g_j = \\nabla L_j = \\sum (y-p)*x_j$</b>\n",
    "\n",
    "(This is the 1st derivative, where $j$ indicates feature dimension $j$)\n",
    "\n",
    "<b> $H_{jk} = \\sum p*(1-p)*x_j*x_k$</b>\n",
    "\n",
    "(This is the 2nd derivative w.r.t. features $j$ and $k$)\n",
    "\n",
    "Now the general form of Gradient Descent follows this function:\n",
    "\n",
    "<b>$w_{new} = w_{old} - \\nu * g$</b>\n",
    "\n",
    "Where $w$ is the weight and $\\nu$ is an appropriately chosen step size. In our case, we're going to use the inverse of the 2nd derviative matrix (the Hessian) as our learning rate. If we define $H$ as the Hessian matrix with entries $H_{jk}$ from above, and $G$ a gradient vector who's $jth$ entry is $g_j$, then our optimization problem becomes:\n",
    "\n",
    "<b>$w_{new} = w_{old} - H^{-1} * G$</b>\n",
    "\n",
    "This is what we are going to program here. Let's define a class that has the following methods:\n",
    "\n",
    "1. An __init__ method that takes in an error tolerence as a stopping criterion, as well as max number of iterations.\n",
    "2. A <b>predict</b> method that takes a given matrix X and predicts $p=(1+e^{(-X*B)})^{-1}$ for each entry\n",
    "3. A <b>compute_gradient</b> method that computes the gradient vector $G$\n",
    "4. A <b>compute_hessian</b> method that computes the Hessian. Note that the $H$ can be broken down to the following matrix multiplcation: $H=X^TQX$, where $X$ is the input matrix and $Q$ is a diagonal matrix where each entry $Q_{ii}=p_i*(1-p_i)$.\n",
    "5. An <b>update_weights</b> method that applies Newton's method to update the weights\n",
    "6. A <b>check_stop</b> method that checks whether the model has converged or the max iterations have been met\n",
    "7. A <b>fit</b> method that takes in the data and runs the gradient optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LinearRegression(object):\n",
    "    \n",
    "    def __init__(self, tol = 10**-8, max_iterations = 20):\n",
    "        \n",
    "        self.tolerance = tol\n",
    "        self.max_iterations = max_iterations\n",
    "        self.beta = None\n",
    "        self.alpha = 0\n",
    "        \n",
    "    def predict(self, Xint):\n",
    "        '''\n",
    "        Compute probs using the inverse logit\n",
    "        - Inputs: The NxK X matrix\n",
    "        - Outputs: Vector of probs of length N\n",
    "        '''\n",
    "        \n",
    "        #First compute X*beta+alpha\n",
    "        XB = Xint.dot(self.b) \n",
    "        return (1+np.exp(-1*XB))**-1\n",
    "        \n",
    "    def compute_gradient(self, Xint, y, p):\n",
    "        '''\n",
    "        Computes the gradient vector\n",
    "        -Inputs:\n",
    "            - NxK X matrix\n",
    "            - Nx1 y (label) vector\n",
    "            - Nx1 ps vector of predictions\n",
    "        -Outputs: 1xK vector of gradients\n",
    "        '''\n",
    "        return (y-p).dot(Xint)\n",
    "        \n",
    "    def compute_hessian(self, Xint, P):\n",
    "        '''\n",
    "        computes the Hessian matrix\n",
    "        -inputs:\n",
    "            - NxK X matrix\n",
    "            - Nx1 vector of predictions\n",
    "        -outputs:\n",
    "            - KxK Hessian matrix H=X^T * Diag(Q) * X\n",
    "        '''        \n",
    "        Q = np.diag(P*(1-P))\n",
    "        return (Xint.T).dot(Q).dot(Xint)\n",
    "\n",
    "\n",
    "    def update_weights(self, Xint, y, i):\n",
    "        '''\n",
    "        Updates existing weight vector\n",
    "        -Inputs:\n",
    "            -NxK X matrix\n",
    "            -Nx1 y vector\n",
    "        -updates weights by calling predict, compute_gradient and compute_hessian\n",
    "        '''\n",
    "        p = self.predict(Xint)\n",
    "        g = self.compute_gradient(Xint, y, p)\n",
    "        H = self.compute_hessian(Xint, p)\n",
    "                \n",
    "        #Store the current weights before updating so we can check for convergence\n",
    "        self.prior_b = self.b\n",
    "        \n",
    "        #update the weights\n",
    "        self.b = self.b + np.linalg.inv(H).dot(g)\n",
    "        \n",
    "        \n",
    "    def check_stop(self):\n",
    "        '''\n",
    "        check to see if euclidean distance between old and new weights (normalized)\n",
    "        is less than the tolerance\n",
    "        \n",
    "        returns: True or False on whether stopping criteria is met\n",
    "        '''\n",
    "        b_old_norm = self.prior_b / (np.sqrt(self.prior_b.dot(self.prior_b)))\n",
    "        b_new_norm = self.b / (np.sqrt(self.b.dot(self.b)))\n",
    "        diff = b_new_norm - b_old_norm\n",
    "        return (np.sqrt(diff.dot(diff)) < self.tolerance)\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        X is the Nx(K-1) data matrix\n",
    "        Y is the labels, using {0,1} coding\n",
    "        '''\n",
    "        \n",
    "        #set initial weights - add an extra dimension for the intercept\n",
    "        self.b = np.zeros(X.shape[1] + 1)\n",
    "        \n",
    "        #Initialize the slope parameter to log(base rate/(1-base rate))\n",
    "        self.b[-1] = np.log(y.mean() / (1-y.mean()))\n",
    "        \n",
    "        #create a new X matrix that includes a column of ones for the intercept\n",
    "        Xint = np.hstack((X, np.ones((X.shape[0],1))))\n",
    "\n",
    "        for i in range(self.max_iterations):\n",
    "            #print(i)\n",
    "            self.update_weights(Xint, y, i)\n",
    "            self.beta = self.b[0:-1]\n",
    "            self.alpha = self.b[-1]\n",
    "            if self.check_stop():\n",
    "                break      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note about testing\n",
    "\n",
    "One way we can test this implementation is to generate some random data according to a logistic model, and see if our model returns the correct weights. To do this:\n",
    "\n",
    "\n",
    "* Generate an NxK X matrix of random numbers\n",
    "* Generate a 1xK weight vector called Beta\n",
    "* Set alpha\n",
    "* Given Beta and Alpha, compute P(Y|X) using the logistic function\n",
    "* Generate an Nx1 random sequence in [0,1], and set Y=1 if R_i < P_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a dataset with N=10k and K=5\n",
    "def gen_logistic(N, K, Beta, Alpha):\n",
    "    X = np.random.random((N,K))\n",
    "    XB = X.dot(Beta) + Alpha * np.ones(N)\n",
    "    P = (1 + np.exp(-1*XB))**-1\n",
    "    Y = (np.random.random(N) < P)\n",
    "    return X, Y\n",
    "\n",
    "K = 2\n",
    "\n",
    "Beta = 2*(np.random.random(K)-1)\n",
    "Alpha = -1\n",
    "\n",
    "X, Y = gen_logistic(1000, K, Beta, Alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, let's test out our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The real Betas and Alpha are:\n",
      "[-1.08280652 -1.88583088] -1\n",
      "\n",
      "The fitted Betas and Alpha are:\n",
      "[-1.47728517 -2.39795267] -0.514633291304\n"
     ]
    }
   ],
   "source": [
    "print('The real Betas and Alpha are:')\n",
    "print(Beta, Alpha)\n",
    "print('')\n",
    "print('The fitted Betas and Alpha are:')\n",
    "print(lr.beta, lr.alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare our fitted results to SkLearn's Logistic Regression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.47728464, -2.39795253]]), array([-0.51463327]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR_SK = LogisticRegression(C=10**10).fit(X,Y)\n",
    "LR_SK.coef_, LR_SK.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is pretty close, though not exact at all digits of precision.  We can also see that both systems produce estimates that are far from the truth. Let us write a more robust test by running multiple draws, and seeing if on average we get the right answer. We can also increase the sample size. Doing either will be more computationally expensive. Before going there let's profile our code to see if there is a way to optimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         275 function calls in 0.010 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        6    0.000    0.000    0.000    0.000 <ipython-input-1-555ed32e48ef>:13(predict)\n",
      "        6    0.000    0.000    0.000    0.000 <ipython-input-1-555ed32e48ef>:24(compute_gradient)\n",
      "        6    0.000    0.000    0.009    0.001 <ipython-input-1-555ed32e48ef>:35(compute_hessian)\n",
      "        6    0.000    0.000    0.010    0.002 <ipython-input-1-555ed32e48ef>:48(update_weights)\n",
      "        6    0.000    0.000    0.000    0.000 <ipython-input-1-555ed32e48ef>:67(check_stop)\n",
      "        1    0.000    0.000    0.010    0.010 <ipython-input-1-555ed32e48ef>:80(fit)\n",
      "        1    0.000    0.000    0.010    0.010 <string>:1(<module>)\n",
      "        2    0.000    0.000    0.000    0.000 _methods.py:43(_count_reduce_items)\n",
      "        2    0.000    0.000    0.000    0.000 _methods.py:53(_mean)\n",
      "        6    0.000    0.000    0.000    0.000 linalg.py:101(get_linalg_error_extobj)\n",
      "        6    0.000    0.000    0.000    0.000 linalg.py:106(_makearray)\n",
      "       12    0.000    0.000    0.000    0.000 linalg.py:111(isComplexType)\n",
      "        6    0.000    0.000    0.000    0.000 linalg.py:124(_realType)\n",
      "        6    0.000    0.000    0.000    0.000 linalg.py:139(_commonType)\n",
      "        6    0.000    0.000    0.000    0.000 linalg.py:198(_assertRankAtLeast2)\n",
      "        6    0.000    0.000    0.000    0.000 linalg.py:209(_assertNdSquareness)\n",
      "        6    0.000    0.000    0.000    0.000 linalg.py:449(inv)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:150(ones)\n",
      "        6    0.000    0.000    0.000    0.000 numeric.py:463(asarray)\n",
      "       10    0.000    0.000    0.000    0.000 numeric.py:534(asanyarray)\n",
      "        2    0.000    0.000    0.000    0.000 shape_base.py:11(atleast_1d)\n",
      "        1    0.000    0.000    0.000    0.000 shape_base.py:239(hstack)\n",
      "        1    0.000    0.000    0.000    0.000 shape_base.py:288(<listcomp>)\n",
      "        6    0.000    0.000    0.003    0.000 twodim_base.py:191(diag)\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method builtins.abs}\n",
      "        1    0.000    0.000    0.010    0.010 {built-in method builtins.exec}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "       20    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "       16    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.array}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.concatenate}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.copyto}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.empty}\n",
      "        7    0.003    0.000    0.003    0.000 {built-in method numpy.core.multiarray.zeros}\n",
      "        6    0.000    0.000    0.000    0.000 {method '__array_prepare__' of 'numpy.ndarray' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       48    0.006    0.000    0.006    0.000 {method 'dot' of 'numpy.ndarray' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "cProfile.run('lr.fit(X, Y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick test, where we design an alternative, and hopefully faster way to compute the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_slow(X, P):\n",
    "    '''\n",
    "    Copy the operations used in the class above\n",
    "    '''\n",
    "    Q = np.diag(P*(1-P))\n",
    "    return (X.T).dot(Q).dot(X)\n",
    "    \n",
    "    \n",
    "def hessian_fast(X, P):\n",
    "    '''\n",
    "    Rewrite this without using the np.diag function\n",
    "    '''\n",
    "    Q = P*(1-P)\n",
    "    XQ = X.T * Q\n",
    "    return XQ.dot(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706 µs ± 5.41 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "P = 0.5 * np.ones(X.shape[0])\n",
    "%timeit hessian_slow(X, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.5 µs ± 270 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hessian_fast(X, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new class, same as above, but overwrite the compute_hessian method with the above faster version. Here we are essentially inherit LinearRegression class, which means all of the methods in the base class are callable for this one. We can then overwrite the compute_hessian method with our faster approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastLinearRegression(LinearRegression):\n",
    "    \n",
    "    def compute_hessian(self, Xint, p):\n",
    "        '''\n",
    "        computes the Hessian matrix\n",
    "        -inputs:\n",
    "            - NxK X matrix\n",
    "            - Nx1 vector of predictions\n",
    "        -outputs:\n",
    "            - KxK Hessian matrix H=X^T * Diag(Q) * X\n",
    "        '''\n",
    "        Q = p*(1-p)\n",
    "        XintQ = Xint.T * Q\n",
    "        return XintQ.dot(Xint)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now rerun\n",
    "lr = FastLinearRegression()\n",
    "lr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.47728517, -2.39795267]), -0.51463329130350843)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.beta, lr.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one last speed test, let's compare our faster class to sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674 µs ± 15.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit LogisticRegression().fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447 µs ± 27.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit FastLinearRegression().fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got an efficient class written, let's perform 3 different unit tests:\n",
    "* Compare the results to SkLearn. This replicates a scenario where for some reason we can't use the SkLearn package and had to write our own code. Since SkLearn does exist, we can at least benchmark against it.\n",
    "* Run it once just on a much larger data set (which should come close to the truth value).\n",
    "* Run it many times over different draws from the same data generating distribution and look at the distribution of outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First let's generate a dataset with 1000000 examples\n",
    "K = 4\n",
    "Beta = 2*(np.random.random(K)-1)\n",
    "Alpha = -2\n",
    "\n",
    "X, Y = gen_logistic(1000000, K, Beta, Alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_SK = LogisticRegression(C=10**30).fit(X,Y)\n",
    "LR_Mine = FastLinearRegression()\n",
    "LR_Mine.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth: beta=[[-1.45742041 -0.31046286 -0.03544492 -0.37854147]], Alpha=-2\n",
      "SkLearn: beta=[[-1.45742041 -0.31046286 -0.03544492 -0.37854147]], Alpha=-2\n",
      "Ours: beta=[-1.45711961 -0.31002732 -0.03523207 -0.37809715], Alpha=-1.97358800177\n"
     ]
    }
   ],
   "source": [
    "print('Truth: beta=' + str(LR_SK.coef_) + ', Alpha=' +str(Alpha))\n",
    "print('SkLearn: beta=' + str(LR_SK.coef_) + ', Alpha=' +str(Alpha))\n",
    "print('Ours: beta=' + str(LR_Mine.beta) + ', Alpha=' +str(LR_Mine.alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this test we'll check whether on average our own LR class is correct. To do this, run a loop where in each iteration, generate a random data set of size N, using the same Beta and Alpha. Then fit the model and store the weights. After this runs we can look at the distributions of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = []\n",
    "alphas = []\n",
    "\n",
    "for i in range(10000):\n",
    "    X, Y = gen_logistic(10000, K, Beta, Alpha)\n",
    "    LR_Mine = FastLinearRegression()\n",
    "    LR_Mine.fit(X, Y)\n",
    "    betas.append(LR_Mine.beta)\n",
    "    alphas.append(LR_Mine.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the histograms of each parameter distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b0      -1.446195\n",
       " b1      -0.292639\n",
       " b2      -0.028182\n",
       " b3      -0.370929\n",
       " alpha   -2.003789\n",
       " dtype: float64,\n",
       " array([-1.44322554, -0.29207703, -0.0283608 , -0.37049464]),\n",
       " -2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(betas, columns = ['b' + str(k) for k in range(K)])\n",
    "df['alpha'] = alphas\n",
    "df.mean(), Beta, Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "bmeans = df.mean()\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "for i in range(K):\n",
    "    fig.add_subplot(2,2,i+1)\n",
    "    plt.hist(df['b'+str(i)], bins=100)\n",
    "    plt.plot()\n",
    "    plt.title('b {}: Fit={}, Truth={}'.format(i, np.round(bmeans[i],2), np.round(Beta[i],2)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
